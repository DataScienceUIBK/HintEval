# This CITATION.cff file was generated with cffinit.
# Visit https://bit.ly/cffinit to generate yours today!

cff-version: 1.2.0
title: HintEval
message: >-
  If you use this software, please cite it using the
  metadata from this file.
type: software
authors:
  - given-names: Jamshid
    family-names: Mozafari
    email: jamshid.mozafari@uibk.ac.at
    affiliation: University of Innsbruck
    orcid: 'https://orcid.org/0000-0003-4850-9239'
  - given-names: Bhawna
    family-names: Piryani
    email: bhawna.piryani@uibk.ac.at
    affiliation: University of Innsbruck
    orcid: 'https://orcid.org/0009-0005-3578-2393'
  - given-names: Abdelrahman
    family-names: Abdallah
    email: abdelrahman.abdallah@uibk.ac.at
    affiliation: University of Innsbruck
    orcid: 'https://orcid.org/0000-0001-8747-4927'
  - given-names: Adam
    family-names: Jatowt
    email: adam.jatowt@uibk.ac.at
    orcid: 'https://orcid.org/0000-0001-7235-0665'
    affiliation: University of Innsbruck
identifiers:
  - type: doi
    value: 10.48550/arXiv.2502.00857
repository-code: 'https://github.com/DataScienceUIBK/HintEval'
abstract: >-
  Large Language Models (LLMs) are transforming how people
  find information, and many users turn nowadays to chatbots
  to obtain answers to their questions. Despite the instant
  access to abundant information that LLMs offer, it is
  still important to promote critical thinking and
  problem-solving skills. Automatic hint generation is a new
  task that aims to support humans in answering questions by
  themselves by creating hints that guide users toward
  answers without directly revealing them. In this context,
  hint evaluation focuses on measuring the quality of hints,
  helping to improve the hint generation approaches.
  However, resources for hint research are currently
  spanning different formats and datasets, while the
  evaluation tools are missing or incompatible, making it
  hard for researchers to compare and test their models. To
  overcome these challenges, we introduce HintEval, a Python
  library that makes it easy to access diverse datasets and
  provides multiple approaches to generate and evaluate
  hints. HintEval aggregates the scattered resources into a
  single toolkit that supports a range of research goals and
  enables a clear, multi-faceted, and reliable evaluation.
  The proposed library also includes detailed online
  documentation, helping users quickly explore its features
  and get started. By reducing barriers to entry and
  encouraging consistent evaluation practices, HintEval
  offers a major step forward for facilitating hint
  generation and analysis research within the NLP/IR
  community.
keywords:
  - Hint Evaluation
  - Hint Generation
  - Python Package
  - Framework
license: Apache-2.0
